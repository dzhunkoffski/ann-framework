{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3fab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d531eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f1d55",
   "metadata": {},
   "source": [
    "## 1. Linear\n",
    "\n",
    "- Prototype: [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- Location in lib: `modules.layers.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_linear ... "
     ]
    }
   ],
   "source": [
    "tests.test_linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4873d3d",
   "metadata": {},
   "source": [
    "## 2. Batch-normalization\n",
    "\n",
    "- Prototype: [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)\n",
    "- Location: `modules.layers.BatchNormalization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_bn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebaf65",
   "metadata": {},
   "source": [
    "## 3. Dropout\n",
    "\n",
    "- Prototype: [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
    "- Location: `modules.layers.Dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fcfb0",
   "metadata": {},
   "source": [
    "## 4. Activation functions\n",
    "\n",
    "### ReLU\n",
    "\n",
    "- Prototype: [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Location: `modules.activations.ReLU`\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "- Prototype: [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html?highlight=nn%20sigmoid#torch.nn.Sigmoid)\n",
    "- Location: `modules.activations.Sigmoid`\n",
    "\n",
    "### Softmax\n",
    "\n",
    "- Prototype: [nn.Softmax](http://bit.ly/get3a)\n",
    "- Location: `modules.activations.Softmax`\n",
    "\n",
    "### LogSoftmax\n",
    "\n",
    "- Prototype: [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html?highlight=log%20softmax#torch.nn.LogSoftmax)\n",
    "- Location: `modules.activations.LogSoftmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a20cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tests.test_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73970a3c",
   "metadata": {},
   "source": [
    "## 5. Sequential\n",
    "\n",
    "- Prototype: [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "- Location: `modules.layers.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1659c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tests.test_sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec537bd",
   "metadata": {},
   "source": [
    "## 6. Loss functions\n",
    "\n",
    "### MSE\n",
    "\n",
    "- Prototype: [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "- Location: `modules.criterions.MSELoss`\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "- Prototype: [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- Location: `modules.criterions.CrossEntropyLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_criterions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d24e6",
   "metadata": {},
   "source": [
    "## 7. Optimizers\n",
    "\n",
    "### SGD\n",
    "\n",
    "- Prototype: [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "- Location: `modules.optimizers.SGD`\n",
    "\n",
    "### Adam\n",
    "\n",
    "- Prototype: [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "- Location: `modules.criterions.CrossEntropyLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec959a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests.test_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ef072",
   "metadata": {},
   "source": [
    "## 8. DataLoader\n",
    "\n",
    "- Prototype: [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- Location: `modules.dataloader.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2064a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2b83f",
   "metadata": {},
   "source": [
    "# Compare with Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7704d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modules as mm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a4bf7",
   "metadata": {},
   "source": [
    "## Test on synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dae81f5",
   "metadata": {},
   "source": [
    "**Mine implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train = np.random.randn(2048, 8)\n",
    "X_test = np.random.randn(512, 8)\n",
    "y_train = np.sin(X_train).sum(axis=1, keepdims=True)\n",
    "y_test = np.sin(X_test).sum(axis=1, keepdims=True)\n",
    "\n",
    "train_loader = mm.DataLoader(X_train.astype('float32'), y_train.astype('float32'), batch_size=64, shuffle=True)\n",
    "test_loader = mm.DataLoader(X_test.astype('float32'), y_test.astype('float32'), batch_size=64, shuffle=False)\n",
    "\n",
    "model = mm.Sequential(\n",
    "    mm.Linear(8, 32),\n",
    "    mm.BatchNormalization(32),\n",
    "    mm.ReLU(),\n",
    "    mm.Linear(32, 64),\n",
    "    mm.Dropout(0.25),\n",
    "    mm.Sigmoid(),\n",
    "    mm.Linear(64, 1)\n",
    ")\n",
    "optimizer = mm.Adam(model, lr=1e-2)\n",
    "criterion = mm.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "pbar = tqdm(range(1, num_epochs + 1))\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        model.backward(X_batch, criterion.backward(predictions, y_batch))\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    model.eval()\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        test_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    train_loss /= train_loader.num_samples()\n",
    "    test_loss /= test_loader.num_samples()\n",
    "    pbar.set_postfix({'train loss': train_loss, 'test loss': test_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1f096",
   "metadata": {},
   "source": [
    "**Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "pbar = tqdm(range(1, num_epochs + 1))\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        y_batch = torch.from_numpy(y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        train_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    model.eval()\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = torch.from_numpy(X_batch)\n",
    "        y_batch = torch.from_numpy(y_batch)\n",
    "        \n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss = loss.item()\n",
    "        test_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    train_loss /= train_loader.num_samples()\n",
    "    test_loss /= test_loader.num_samples()\n",
    "    pbar.set_postfix({'train loss': train_loss, 'test loss': test_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15cdb6",
   "metadata": {},
   "source": [
    "## MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aad33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mnist_df = pd.read_csv('mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab595a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_df.drop(['label'], axis=1)\n",
    "y = mnist_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a970c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[5:6].values[0].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8aa809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9acaa",
   "metadata": {},
   "source": [
    "Code below will visualize training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_losses(train_losses, train_metrics, val_losses, val_metrics):\n",
    "    clear_output()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n",
    "    axs[0].plot(range(1, len(val_losses) + 1), val_losses, label='val')\n",
    "    axs[1].plot(range(1, len(train_metrics) + 1), train_metrics, label='train')\n",
    "    axs[1].plot(range(1, len(val_metrics) + 1), val_metrics, label='val')\n",
    "\n",
    "    if max(train_losses) / min(train_losses) > 10:\n",
    "        axs[0].set_yscale('log')\n",
    "\n",
    "    if max(train_metrics) / min(train_metrics) > 10:\n",
    "        axs[0].set_yscale('log')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_and_validate(model, optimizer, criterion, metric, train_loader, val_loader,\n",
    "                       num_epochs, verbose=True, use_torch = False):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_metrics, val_metrics = [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss, running_metric = 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f'Training {epoch}/{num_epochs}') \\\n",
    "            if verbose else train_loader\n",
    "\n",
    "        for X_batch, y_batch in pbar:\n",
    "            \n",
    "            if use_torch:\n",
    "                X_batch = torch.from_numpy(X_batch)\n",
    "                y_batch = torch.from_numpy(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            if use_torch:\n",
    "                loss.backward()\n",
    "            else:\n",
    "                model.backward(X_batch, criterion.backward(predictions, y_batch))\n",
    "            optimizer.step()\n",
    "            \n",
    "            metric_value = metric(predictions, y_batch)\n",
    "            if use_torch:\n",
    "                loss = loss.item()\n",
    "                metric_value = metric_value.item()\n",
    "            running_loss += loss * X_batch.shape[0]\n",
    "            running_metric += metric_value * X_batch.shape[0]\n",
    "            if verbose:\n",
    "                pbar.set_postfix({'loss': loss, 'Accuracy': metric_value})\n",
    "\n",
    "        train_losses += [running_loss / train_loader.num_samples()]\n",
    "        train_metrics += [running_metric / train_loader.num_samples()]\n",
    "\n",
    "        model.eval()\n",
    "        running_loss, running_metric = 0, 0\n",
    "        pbar = tqdm(val_loader, desc=f'Validating {epoch}/{num_epochs}') \\\n",
    "            if verbose else val_loader\n",
    "\n",
    "        for X_batch, y_batch in pbar:\n",
    "            \n",
    "            if use_torch:\n",
    "                X_batch = torch.from_numpy(X_batch)\n",
    "                y_batch = torch.from_numpy(y_batch)\n",
    "            \n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            \n",
    "            metric_value = metric(predictions, y_batch)\n",
    "            if use_torch:\n",
    "                loss = loss.item()\n",
    "                metric_value = metric_value.item()\n",
    "            running_loss += loss * X_batch.shape[0]\n",
    "            running_metric += metric_value * X_batch.shape[0]\n",
    "            if verbose:\n",
    "                pbar.set_postfix({'loss': loss, 'Accuracy': metric_value})\n",
    "\n",
    "        val_losses += [running_loss / val_loader.num_samples()]\n",
    "        val_metrics += [running_metric / val_loader.num_samples()]\n",
    "\n",
    "        if verbose:\n",
    "            plot_losses(train_losses, train_metrics, val_losses, val_metrics)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Validation Accuracy: {val_metrics[-1]:.3f}')\n",
    "    \n",
    "    return train_metrics[-1], val_metrics[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e14321",
   "metadata": {},
   "source": [
    "## Train and valdiate with own framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2733fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0316aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return (np.argmax(y_pred, axis=1) == y_true).sum() / y_pred.shape[0]\n",
    "     \n",
    "     \n",
    "model = mm.Sequential(\n",
    "    mm.Linear(in_features=IMG_SIZE**2, out_features=HIDDEN_SIZE),\n",
    "    mm.ReLU(),\n",
    "    mm.Linear(in_features=HIDDEN_SIZE, out_features=HIDDEN_SIZE),\n",
    "    mm.ReLU(),\n",
    "    mm.Linear(in_features=HIDDEN_SIZE, out_features=NUM_CLASSES),\n",
    ")\n",
    "\n",
    "dataloader_train = mm.DataLoader(X_train.values, y_train.values, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_val = mm.DataLoader(X_val.values , y_val.values, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = mm.optimizers.Adam(model, lr=1e-3)\n",
    "criterion = mm.CrossEntropyLoss()\n",
    "train_metric, val_metric = train_and_validate(model, optimizer, criterion, accuracy, dataloader_train, dataloader_val,\n",
    "                       NUM_EPOCHS, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ea06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
